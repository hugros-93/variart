{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vari'art: \n",
    "### Example of latent analysis of a rap clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    InputLayer, \n",
    "    Dense, \n",
    "    Reshape, \n",
    "    Flatten, \n",
    "    Dropout, \n",
    "    Conv2D, \n",
    "    Conv2DTranspose, \n",
    "    MaxPool2D,\n",
    "    BatchNormalization,\n",
    "    LeakyReLU\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from variart.preprocessing import ArtVideo\n",
    "from variart.model import VAE, GAN\n",
    "from variart.latent import Latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video\n",
    "name = 'DrillFR4' \n",
    "filename = 'inputs/DrillFR4.mp4'\n",
    "DrillFR4 = ArtVideo(name, filename)\n",
    "DrillFR4.load_video()\n",
    "\n",
    "# Crop images as squares\n",
    "DrillFR4.square()\n",
    "\n",
    "# Resize images\n",
    "size = 128\n",
    "new_shape=(size,size)\n",
    "DrillFR4.resize(new_shape=new_shape)\n",
    "\n",
    "# Rescale pixels in (0,1)\n",
    "DrillFR4.rescale_images()\n",
    "\n",
    "# Input data shape\n",
    "print(f\"Shape {DrillFR4.name}: {DrillFR4.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show randomm image\n",
    "DrillFR4.show_random_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 128\n",
    "noise_dim = 256\n",
    "learning_rate=1e-4\n",
    "wgan=False # Wasserstein GAN configuration if True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "data_train = DrillFR4.X.astype('float32')\n",
    "data_train = shuffle(data_train, random_state=0)\n",
    "\n",
    "input_shape_tuple = data_train.shape[1:]\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(data_train).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of functions to create the generator and the discriminator\n",
    "def make_generator_model():\n",
    "    dim=int(size/4)\n",
    "    generative_net = tf.keras.Sequential(\n",
    "        [\n",
    "            Dense(units=dim*dim*32, use_bias=False, input_shape=(noise_dim,)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            Reshape(target_shape=(dim, dim, 32)),\n",
    "            Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', use_bias=False),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', use_bias=False),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            Conv2DTranspose(filters=3, kernel_size=3, strides=1, padding='same', use_bias=False, activation='tanh'),\n",
    "        ]\n",
    "    )\n",
    "    return generative_net\n",
    "\n",
    "def make_discriminator_model(wgan=False):\n",
    "    discriminative_net = tf.keras.Sequential([\n",
    "        Conv2D(16, (5, 5), strides=(2, 2), padding='same', input_shape=[size, size, 3]),\n",
    "        LeakyReLU(),\n",
    "        Dropout(0.3),\n",
    "        Conv2D(32, (5, 5), strides=(2, 2), padding='same'),\n",
    "        LeakyReLU(),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "    ])\n",
    "    \n",
    "    if wgan:\n",
    "        discriminative_net.add(Dense(1))\n",
    "    else:\n",
    "        discriminative_net.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return discriminative_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genertor and discriminator\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model(wgan=wgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GAN object\n",
    "gan_model = GAN(DrillFR4.name, noise_dim, input_shape_tuple, generator, discriminator, learning_rate=learning_rate, wgan=wgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train GAN\n",
    "gan_model.train(train_dataset, epochs=1000, n_steps_gen=1, n_steps_disc=1, freq_plot=10, n_to_plot=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images\n",
    "gan_model.generate_and_plot(n_to_plot=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "data = DrillFR4.X.astype('float32')\n",
    "data = shuffle(data, random_state=0)\n",
    "\n",
    "TRAIN_BUF = int(data.shape[0]*0.9)\n",
    "data_train = data[:TRAIN_BUF]\n",
    "data_validation = data[TRAIN_BUF:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 128\n",
    "epochs = 10000\n",
    "early_stop_patience = 15\n",
    "latent_dim = 16\n",
    "optimizer = Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(data_train).batch(batch_size)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(data_validation).batch(batch_size)\n",
    "nb_features = data.shape[1]*data.shape[2]*data.shape[3]\n",
    "input_shape = (batch_size, data.shape[1], data.shape[2], data.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and decoder networks (inference and generative)\n",
    "inference_net = tf.keras.Sequential(\n",
    "      [\n",
    "          InputLayer(input_shape=(data.shape[1], data.shape[2], data.shape[3])),\n",
    "          Conv2D(filters=4, kernel_size=3, strides=(1, 1), activation='tanh'),\n",
    "          MaxPool2D((2,2)),\n",
    "          BatchNormalization(),\n",
    "          Conv2D(filters=8, kernel_size=3, strides=(1, 1), activation='tanh'),\n",
    "          MaxPool2D((2,2)),\n",
    "          BatchNormalization(),\n",
    "          Flatten(),\n",
    "          Dense(latent_dim + latent_dim),\n",
    "      ]\n",
    "    )\n",
    "\n",
    "generative_net = tf.keras.Sequential(\n",
    "        [\n",
    "            InputLayer(input_shape=(latent_dim,)),\n",
    "            Dense(units=data.shape[1]*data.shape[2]*4, activation='tanh'),\n",
    "            BatchNormalization(),\n",
    "            Reshape(target_shape=(data.shape[1], data.shape[2], 4)),\n",
    "            Conv2DTranspose(\n",
    "              filters=8,\n",
    "              kernel_size=3,\n",
    "              strides=(1, 1),\n",
    "              padding=\"SAME\",\n",
    "              activation='tanh'),\n",
    "            BatchNormalization(),\n",
    "            Conv2DTranspose(\n",
    "              filters=4,\n",
    "              kernel_size=3,\n",
    "              strides=(1, 1),\n",
    "              padding=\"SAME\",\n",
    "              activation='tanh'),\n",
    "            BatchNormalization(),\n",
    "            Conv2DTranspose(\n",
    "              filters=3, kernel_size=3, strides=(1, 1), padding=\"SAME\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = VAE(DrillFR4.name, latent_dim, input_shape, inference_net, generative_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model = model.train(optimizer, \n",
    "                    train_dataset, \n",
    "                    validation_dataset, \n",
    "                    epochs,\n",
    "                    batch_size,\n",
    "                    early_stop_patience = early_stop_patience, \n",
    "                    freq_plot = 25, \n",
    "                    plot_test = True,\n",
    "                    n_to_plot = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create latent object\n",
    "LatentDrillFR4 = Latent(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and decode data\n",
    "LatentDrillFR4.encode_data()\n",
    "LatentDrillFR4.decode_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tsne representation of data in latent space\n",
    "LatentDrillFR4.latent_tsne()\n",
    "LatentDrillFR4.plot_latent_tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distributions of latent space dimensions\n",
    "LatentDrillFR4.compute_dist_coord()\n",
    "LatentDrillFR4.plot_latent_dist_coord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering in latent space, test number of cluesters on a grid\n",
    "LatentDrillFR4.latent_space_clustering(grid=range(5,100,5))\n",
    "LatentDrillFR4.plot_silhouette_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select number of clusters\n",
    "n_clusters = 5\n",
    "clusterer = LatentDrillFR4.dico_clust[n_clusters]['clusterer']\n",
    "LatentDrillFR4.plot_latent_tsne(clusterer=clusterer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show images for a given cluster\n",
    "label = 0\n",
    "list_id = [i for i,l in enumerate(clusterer.labels_) if l==label][0:5]\n",
    "LatentDrillFR4.plot_encoded_decoded(list_id=list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images by sampling from distributions in the latent space\n",
    "list_z, fig = LatentDrillFR4.generate_image(n=5, method='dist')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GIF from generated images\n",
    "filename = f\"outputs/gif_{LatentDrillFR4.name}.gif\"\n",
    "LatentDrillFR4.create_gif(list_z, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}